{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    MNIST\\n    DataLoaer, Transformation\\n    Multilayer Neural Net, activation function\\n    loss and optimizer\\n    training loop(batch training)\\n    model evaluation\\n    GPU SUPPORT\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MNIST\n",
    "    DataLoaer, Transformation\n",
    "    Multilayer Neural Net, activation function\n",
    "    loss and optimizer\n",
    "    training loop(batch training)\n",
    "    model evaluation\n",
    "    GPU SUPPORT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TENSORBOARD ########################\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist1')\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"hello\")\n",
    "else:\n",
    "    print(\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_size = 784#28*28\n",
    "hidden_size =100\n",
    "num_classes =10# number is from 0 to 9\n",
    "num_epochs = 20\n",
    "batch_size=100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist\n",
    "train_dataset = torchvision.datasets.MNIST(root = './',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root = './',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples,labels = examples.next()\n",
    "print(samples.shape,labels.shape)\n",
    "# batch size is 100, 1- because we have single colour channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TENSORBOARD ########################\n",
    "img_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images', img_grid)\n",
    "#writer.close()\n",
    "#sys.exit()\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "tensor([2, 2, 1, 6, 8, 2, 4, 8, 1, 7, 8, 6, 6, 9, 6, 0, 9, 6, 5, 2, 5, 3, 1, 8,\n",
      "        9, 1, 6, 5, 8, 5, 3, 8, 8, 5, 7, 3, 4, 4, 9, 9, 2, 2, 9, 4, 5, 1, 0, 6,\n",
      "        8, 0, 1, 7, 1, 1, 0, 3, 1, 4, 1, 6, 0, 4, 0, 2, 2, 3, 7, 7, 3, 5, 8, 4,\n",
      "        4, 8, 7, 3, 2, 1, 2, 2, 1, 6, 7, 8, 5, 5, 9, 5, 7, 0, 7, 3, 2, 7, 4, 4,\n",
      "        2, 6, 8, 0])\n"
     ]
    }
   ],
   "source": [
    "print(samples)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAANIUlEQVR4nO3db6wU9b3H8c9HLzwQiEHNPRJKbEUN1htrDZKbSG4wtUTQBKsJguSGm5h7MKlNG4sp8T6oD/yXpn+iiWlyKgTatNQmByIh9V6QNOJ9YMNBUVDT+idgOUFo9QHUaHqB731wBnPU3dlzdnZ3Fr7vV3Kyu/Pdmfky8GFmZ2bPzxEhAOe+8+puAEBvEHYgCcIOJEHYgSQIO5DEP/VyZbY59Q90WUS40fRKe3bbt9j+k+23ba+rsiwA3eV2r7PbPl/SnyV9U9JhSXskrYyIN0rmYc8OdFk39uwLJL0dEe9GxD8k/VbSsgrLA9BFVcI+W9Jfxr0+XEz7DNuDtkdsj1RYF4CKun6CLiKGJA1JHMYDdaqyZx+VNGfc6y8V0wD0oSph3yPpSttfsT1V0gpJ2zrTFoBOa/swPiJO2r5P0v9IOl/Shoh4vWOdAeioti+9tbUyPrMDXdeVm2oAnD0IO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0m0PT67JNk+KOmEpFOSTkbE/E40BaDzKoW9cFNE/K0DywHQRRzGA0lUDXtI2mF7r+3BRm+wPWh7xPZIxXUBqMAR0f7M9uyIGLX9z5J2SvpOROwueX/7KwMwIRHhRtMr7dkjYrR4PCZpq6QFVZYHoHvaDrvtabZnnHkuabGkA51qDEBnVTkbPyBpq+0zy/lNRPx3R7rCWePaa68trS9durRp7Zprrimdd9WqVaX1e++9t7Q+NDRUWs+m7bBHxLuSvtbBXgB0EZfegCQIO5AEYQeSIOxAEoQdSKLSHXSTXhl30HXFlClTmtYuvPDC0nlvvvnm0vodd9xRWl+yZElpfdq0aU1rVf/tvfrqq6X166+/vtLyz1ZduYMOwNmDsANJEHYgCcIOJEHYgSQIO5AEYQeS6MQvnESXLVy4sLT+6KOPNq3deOONnW5nUl588cWmtVZ/rlY2b95caf5s2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ++BVr8y+cknnyytt7pWPnXq1Ka1d955p3Te5557rrS+ZcuW0vr+/ftL6/PmzWta27276eBBE7Jnz55K82fDnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA6ew9cccUVpfVFixZVWv5jjz3WtPbwww+Xzvvxxx9XWncrV111VdeW3eoaPz6r5Z7d9gbbx2wfGDftIts7bb9VPM7sbpsAqprIYfxGSbd8bto6Sbsi4kpJu4rXAPpYy7BHxG5JH35u8jJJm4rnmyTd3tm2AHRau5/ZByLiSPH8fUkDzd5oe1DSYJvrAdAhlU/QRUSUDdgYEUOShiQGdgTq1O6lt6O2Z0lS8Xiscy0B6IZ2w75N0uri+WpJz3amHQDd0vIw3vZmSYskXWL7sKQfSnpc0u9s3yPpkKTl3WzybLdv377S+vLl5ZtveHi4g930Vtn47nbDYcQ/9corr5TWP/roo7Z6yqpl2CNiZZPSNzrcC4Au4nZZIAnCDiRB2IEkCDuQBGEHkuArrj1w6NChSvV+tmrVqtL6rbfe2rR26tSp0nm3bt1aWv/kk09K6/gs9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjevfLY/hNNWefGTNmlNZfeuml0vrVV1/dtLZ3797SeW+44YbSOhqLiIbfHWbPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ8H12lFq7dm1pfd68eaX1gwcPtr1sdBZ7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IguvsyV122WWl9bvvvrvS8h944IGmtRdeeKHSsjE5LffstjfYPmb7wLhpD9ketb2v+Fna3TYBVDWRw/iNkm5pMP1nEXFd8fP7zrYFoNNahj0idkv6sAe9AOiiKifo7rP9WnGYP7PZm2wP2h6xPVJhXQAqajfsP5c0V9J1ko5I+kmzN0bEUETMj4j5ba4LQAe0FfaIOBoRpyLitKRfSFrQ2bYAdFpbYbc9a9zLb0k60Oy9APpDy+vstjdLWiTpEtuHJf1Q0iLb10kKSQclrelei+imJ554orQ+d+7c0vr69etL68PDw5PuCd3RMuwRsbLB5PK/YQB9h9tlgSQIO5AEYQeSIOxAEoQdSIKvuJ7jFi9eXFq/6aabSusnT54srT/zzDOT7gn1YM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnf0ccPHFFzetPf3006XzTp8+vbT+1FNPldaff/750jr6B3t2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC6+zngAULmo/RMXv27NJ5T5w4UVp/5JFH2uoJ/Yc9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2c8CGDRvannft2rWl9aNHj7a97HPZbbfdVlrfvn17jzqZuJZ7dttzbP/B9hu2X7f93WL6RbZ32n6reJzZ/XYBtGsih/EnJX0/Ir4q6V8lfdv2VyWtk7QrIq6UtKt4DaBPtQx7RByJiJeL5yckvSlptqRlkjYVb9sk6fYu9QigAyb1md32lyV9XdIfJQ1ExJGi9L6kgSbzDEoarNAjgA6Y8Nl429MlDUv6XkQcH1+LiJAUjeaLiKGImB8R8yt1CqCSCYXd9hSNBf3XEbGlmHzU9qyiPkvSse60CKATWh7G27ak9ZLejIifjittk7Ra0uPF47Nd6RC68847S+uXXnpp09rx48eb1iRpx44dbfWU3cBAw0+tfW0in9lvlPTvkvbb3ldMe1BjIf+d7XskHZK0vCsdAuiIlmGPiP+V5Cblb3S2HQDdwu2yQBKEHUiCsANJEHYgCcIOJMFXXPvA5ZdfXlrfuHFjaX3sBsbG1q0r/37Se++9V1pHY6dPn667hUljzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCdvQ+sWbOmtH7BBReU1j/44IOmtW3btrXVE8497NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAmus/fAihUrSuv3339/peUvWbKkaW10dLTSstFYq9/H34/YswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhMZn32OpF9KGpAUkoYi4gnbD0n6T0l/Ld76YET8vluNns3uuuuu0vp555X/n7t9+/bS+sjIyKR7QjXDw8N1tzBpE7mp5qSk70fEy7ZnSNpre2dR+1lE/Lh77QHolImMz35E0pHi+Qnbb0qa3e3GAHTWpD6z2/6ypK9L+mMx6T7br9neYHtmk3kGbY/Y5lgTqNGEw257uqRhSd+LiOOSfi5prqTrNLbn/0mj+SJiKCLmR8T86u0CaNeEwm57isaC/uuI2CJJEXE0Ik5FxGlJv5C0oHttAqiqZdhtW9J6SW9GxE/HTZ817m3fknSg8+0B6BSXDfcrSbYXSnpR0n5JZ8apfVDSSo0dwoekg5LWFCfzypZVvjIAlUWEG01vGfZOIuxA9zULO3fQAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuj1kM1/k3Ro3OtLimn9qF9769e+JHprVyd7u6xZoaffZ//Cyu2Rfv3ddP3aW7/2JdFbu3rVG4fxQBKEHUii7rAP1bz+Mv3aW7/2JdFbu3rSW62f2QH0Tt17dgA9QtiBJGoJu+1bbP/J9tu219XRQzO2D9reb3tf3ePTFWPoHbN9YNy0i2zvtP1W8dhwjL2aenvI9mix7fbZXlpTb3Ns/8H2G7Zft/3dYnqt266kr55st55/Zrd9vqQ/S/qmpMOS9khaGRFv9LSRJmwflDQ/Imq/AcP2v0n6u6RfRsS/FNN+JOnDiHi8+I9yZkT8oE96e0jS3+sexrsYrWjW+GHGJd0u6T9U47Yr6Wu5erDd6tizL5D0dkS8GxH/kPRbSctq6KPvRcRuSR9+bvIySZuK55s09o+l55r01hci4khEvFw8PyHpzDDjtW67kr56oo6wz5b0l3GvD6u/xnsPSTts77U9WHczDQyMG2brfUkDdTbTQMthvHvpc8OM9822a2f486o4QfdFCyPieklLJH27OFztSzH2Gayfrp1OaBjvXmkwzPin6tx27Q5/XlUdYR+VNGfc6y8V0/pCRIwWj8ckbVX/DUV99MwIusXjsZr7+VQ/DePdaJhx9cG2q3P48zrCvkfSlba/YnuqpBWSttXQxxfYnlacOJHtaZIWq/+Got4maXXxfLWkZ2vs5TP6ZRjvZsOMq+ZtV/vw5xHR8x9JSzV2Rv4dSf9VRw9N+rpc0qvFz+t19yZps8YO6/5PY+c27pF0saRdkt6S9Lyki/qot19pbGjv1zQWrFk19bZQY4for0naV/wsrXvblfTVk+3G7bJAEpygA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk/h+atvdSul4GCgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(samples[9][0],cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to make our classifier model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size,hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.l2 = nn.Linear(hidden_size,hidden_size//2)\n",
    "        self.l3=nn.Linear(hidden_size//2,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.tanh(out)\n",
    "        out=self.l2(out)\n",
    "        out=self.tanh(out)\n",
    "        out=self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size,hidden_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # this will create the softmax for us \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model,example_data.reshape(-1,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "n_total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, step 100/600, loss = 2.2988\n",
      "epoch 1 / 20, step 200/600, loss = 2.2776\n",
      "epoch 1 / 20, step 300/600, loss = 2.3098\n",
      "epoch 1 / 20, step 400/600, loss = 2.3101\n",
      "epoch 1 / 20, step 500/600, loss = 2.3259\n",
      "epoch 1 / 20, step 600/600, loss = 2.3117\n",
      "epoch 2 / 20, step 100/600, loss = 2.2907\n",
      "epoch 2 / 20, step 200/600, loss = 2.3167\n",
      "epoch 2 / 20, step 300/600, loss = 2.2815\n",
      "epoch 2 / 20, step 400/600, loss = 2.3225\n",
      "epoch 2 / 20, step 500/600, loss = 2.3069\n",
      "epoch 2 / 20, step 600/600, loss = 2.2918\n",
      "epoch 3 / 20, step 100/600, loss = 2.3035\n",
      "epoch 3 / 20, step 200/600, loss = 2.2783\n",
      "epoch 3 / 20, step 300/600, loss = 2.3075\n",
      "epoch 3 / 20, step 400/600, loss = 2.3142\n",
      "epoch 3 / 20, step 500/600, loss = 2.2947\n",
      "epoch 3 / 20, step 600/600, loss = 2.2996\n",
      "epoch 4 / 20, step 100/600, loss = 2.3007\n",
      "epoch 4 / 20, step 200/600, loss = 2.3022\n",
      "epoch 4 / 20, step 300/600, loss = 2.3040\n",
      "epoch 4 / 20, step 400/600, loss = 2.3178\n",
      "epoch 4 / 20, step 500/600, loss = 2.3248\n",
      "epoch 4 / 20, step 600/600, loss = 2.3104\n",
      "epoch 5 / 20, step 100/600, loss = 2.3157\n",
      "epoch 5 / 20, step 200/600, loss = 2.3174\n",
      "epoch 5 / 20, step 300/600, loss = 2.3132\n",
      "epoch 5 / 20, step 400/600, loss = 2.3033\n",
      "epoch 5 / 20, step 500/600, loss = 2.3179\n",
      "epoch 5 / 20, step 600/600, loss = 2.2862\n",
      "epoch 6 / 20, step 100/600, loss = 2.3068\n",
      "epoch 6 / 20, step 200/600, loss = 2.2926\n",
      "epoch 6 / 20, step 300/600, loss = 2.3072\n",
      "epoch 6 / 20, step 400/600, loss = 2.2997\n",
      "epoch 6 / 20, step 500/600, loss = 2.3060\n",
      "epoch 6 / 20, step 600/600, loss = 2.3147\n",
      "epoch 7 / 20, step 100/600, loss = 2.2967\n",
      "epoch 7 / 20, step 200/600, loss = 2.3037\n",
      "epoch 7 / 20, step 300/600, loss = 2.3071\n",
      "epoch 7 / 20, step 400/600, loss = 2.3072\n",
      "epoch 7 / 20, step 500/600, loss = 2.3065\n",
      "epoch 7 / 20, step 600/600, loss = 2.2929\n",
      "epoch 8 / 20, step 100/600, loss = 2.3227\n",
      "epoch 8 / 20, step 200/600, loss = 2.3175\n",
      "epoch 8 / 20, step 300/600, loss = 2.2998\n",
      "epoch 8 / 20, step 400/600, loss = 2.2931\n",
      "epoch 8 / 20, step 500/600, loss = 2.3176\n",
      "epoch 8 / 20, step 600/600, loss = 2.2994\n",
      "epoch 9 / 20, step 100/600, loss = 2.3142\n",
      "epoch 9 / 20, step 200/600, loss = 2.3025\n",
      "epoch 9 / 20, step 300/600, loss = 2.2952\n",
      "epoch 9 / 20, step 400/600, loss = 2.3054\n",
      "epoch 9 / 20, step 500/600, loss = 2.3040\n",
      "epoch 9 / 20, step 600/600, loss = 2.3257\n",
      "epoch 10 / 20, step 100/600, loss = 2.2929\n",
      "epoch 10 / 20, step 200/600, loss = 2.3310\n",
      "epoch 10 / 20, step 300/600, loss = 2.2990\n",
      "epoch 10 / 20, step 400/600, loss = 2.3011\n",
      "epoch 10 / 20, step 500/600, loss = 2.2956\n",
      "epoch 10 / 20, step 600/600, loss = 2.3135\n",
      "epoch 11 / 20, step 100/600, loss = 2.2960\n",
      "epoch 11 / 20, step 200/600, loss = 2.3002\n",
      "epoch 11 / 20, step 300/600, loss = 2.2973\n",
      "epoch 11 / 20, step 400/600, loss = 2.3075\n",
      "epoch 11 / 20, step 500/600, loss = 2.3179\n",
      "epoch 11 / 20, step 600/600, loss = 2.3063\n",
      "epoch 12 / 20, step 100/600, loss = 2.3136\n",
      "epoch 12 / 20, step 200/600, loss = 2.3114\n",
      "epoch 12 / 20, step 300/600, loss = 2.2879\n",
      "epoch 12 / 20, step 400/600, loss = 2.2849\n",
      "epoch 12 / 20, step 500/600, loss = 2.3037\n",
      "epoch 12 / 20, step 600/600, loss = 2.2978\n",
      "epoch 13 / 20, step 100/600, loss = 2.3125\n",
      "epoch 13 / 20, step 200/600, loss = 2.2966\n",
      "epoch 13 / 20, step 300/600, loss = 2.2923\n",
      "epoch 13 / 20, step 400/600, loss = 2.3193\n",
      "epoch 13 / 20, step 500/600, loss = 2.3131\n",
      "epoch 13 / 20, step 600/600, loss = 2.2955\n",
      "epoch 14 / 20, step 100/600, loss = 2.3008\n",
      "epoch 14 / 20, step 200/600, loss = 2.3083\n",
      "epoch 14 / 20, step 300/600, loss = 2.3043\n",
      "epoch 14 / 20, step 400/600, loss = 2.3110\n",
      "epoch 14 / 20, step 500/600, loss = 2.3094\n",
      "epoch 14 / 20, step 600/600, loss = 2.3289\n",
      "epoch 15 / 20, step 100/600, loss = 2.2960\n",
      "epoch 15 / 20, step 200/600, loss = 2.3136\n",
      "epoch 15 / 20, step 300/600, loss = 2.2969\n",
      "epoch 15 / 20, step 400/600, loss = 2.2968\n",
      "epoch 15 / 20, step 500/600, loss = 2.3140\n",
      "epoch 15 / 20, step 600/600, loss = 2.2887\n",
      "epoch 16 / 20, step 100/600, loss = 2.2908\n",
      "epoch 16 / 20, step 200/600, loss = 2.3024\n",
      "epoch 16 / 20, step 300/600, loss = 2.3257\n",
      "epoch 16 / 20, step 400/600, loss = 2.3338\n",
      "epoch 16 / 20, step 500/600, loss = 2.3262\n",
      "epoch 16 / 20, step 600/600, loss = 2.3058\n",
      "epoch 17 / 20, step 100/600, loss = 2.3069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Machine Learning\\Pytorch\\14tensorboard.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000016?line=7'>8</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000016?line=9'>10</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000016?line=10'>11</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000016?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000016?line=13'>14</a>\u001b[0m \u001b[39m#backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32me:\\Machine Learning\\Pytorch\\14tensorboard.ipynb Cell 17'\u001b[0m in \u001b[0;36mNeuralNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000013?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000013?line=10'>11</a>\u001b[0m     out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000013?line=11'>12</a>\u001b[0m     out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Machine%20Learning/Pytorch/14tensorboard.ipynb#ch0000013?line=12'>13</a>\u001b[0m     out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2(out)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        # we need to reshape our images\n",
    "        # 100,1,28,28 to  100,784\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) %100==0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "            ############## TENSORBOARD ########################\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / 100 / predicted.size(0)\n",
    "            writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "            running_correct = 0\n",
    "            running_loss = 0.0\n",
    "            ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with torch.no_grad():\\n    n_correct = 0\\n    n_samples = 0\\n    for images, labels in test_loader:\\n        images = images.reshape(-1, 28*28).to(device)\\n        labels = labels.to(device)\\n        outputs = model(images)\\n\\n        # value, index\\n        _, predictions = torch.max(outputs, 1)\\n        n_samples += labels.shape[0]\\n        n_correct += (predictions == labels).sum().item()\\n\\n    acc = 100 * n_correct/n_samples  # calculate accuracy\\n    print(f'accuracy: {acc}')\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\"\"\"with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # value, index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    acc = 100 * n_correct/n_samples  # calculate accuracy\n",
    "    print(f'accuracy: {acc}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 12.37 %\n",
      "count of correct answer was 1237 among 10000 images\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "class_labels = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        values, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "\n",
    "        class_preds.append(class_probs_batch)\n",
    "        class_labels.append(predicted)\n",
    "\n",
    "    # 10000, 10, and 10000, 1\n",
    "    # stack concatenates tensors along a new dimension\n",
    "    # cat concatenates tensors in the given dimension\n",
    "    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n",
    "    class_labels = torch.cat(class_labels)\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n",
    "    ############## TENSORBOARD ########################\n",
    "    classes = range(10)\n",
    "    for i in classes:\n",
    "        labels_i = class_labels == i\n",
    "        preds_i = class_preds[:, i]\n",
    "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
    "        writer.close()\n",
    "    ###################################################\n",
    "    print(f'count of correct answer was {n_correct} among 10000 images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23f823caeccb1684f6bd50492b68bb5eab208de0408f39b2457c6a38c0e8e818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
